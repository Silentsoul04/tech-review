---
title: Python 数据科学手册 5.1 什么是机器学习
id: csdn74001789
---

# 5.1 什么是机器学习

> 原文：[What Is Machine Learning?](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.01-What-Is-Machine-Learning.ipynb)
> 
> 译者：[飞龙](https://github.com/wizardforcel)
> 
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)
> 
> 译文没有得到原作者授权，不保证与原文的意思严格一致。

在我们查看机器学习方法的各种细节之前，先了解什么是机器学习，什么不是。机器学习通常被归类为人工智能的一个子领域，但是我发现分类往往会首先产生误导。机器学习的研究肯定来自于这一背景下的研究，但在机器学习方法的数据科学应用中，将机器学习视为构建数据模型的手段更有帮助。

从根本上讲，机器学习涉及建立数学模型来帮助了解数据。当我们给出这些模型的可调参数，它们可以适应于观测数据时，“学习”就开始了；以这种方式，该程序可以认为是从数据中“学习”。一旦这些模型已经适合以前看到的数据，它们可以用于预测和理解新观测的数据。这种数学的，基于模型的“学习”，类似于人类大脑展示的“学习”，我会向读者提供更多的哲学的题外话。

了解机器学习中的问题设置，对于有效使用这些工具至关重要，因此我们从更广泛的方法分类开始，这些方法会在这里讨论。

## 机器学习的分类

最基本的层次，机器学习可以分为两种主要类型：监督学习和无监督学习。

监督学习涉及以某种方式，对数据的测量特征，以及数据相关的一些标签之间的关系进行建模；一旦确定了该模型，它可以用于给新的未知数据贴标签。这进一步细分为分类任务和回归任务：在分类中，标签是离散类别，而在回归中，标签是连续数量。我们将在下一节中看到两种类型的监督学习的例子。

无监督的学习涉及到对数据集的特征进行建模，而不参考任何标签，并且通常被描述为“让数据集本身说话”。这些模型包括例如聚类和降维的任务。聚类算法识别不同的数据分组，而降维算法搜索数据的更简洁表示。我们将在下一节中看到两种类型的无监督学习的例子。

此外，还有所谓的半监督学习方法，属于监督学习与无监督学习之间。当只有不完整的标签可用时，半监督学习方法通​​常是有用的。

## 机器学习应用的定性实例

为了使这些概念更具体，让我们来看看机器学习任务的一些非常简单的例子。 这些例子的目的是，给出本章将要介绍的机器学习任务类型的直观，非定量的概述。 在后面的章节中，我们将对特定模型及其使用方式进行更深入的研究。 有关更多技术方面的预览，你可以在[附录：图形代码](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/06.00-Figure-Code.ipynb)中，找到生成以下图形的 Python 源代码。

### 分类：预测离散标签

我们将首先看一个简单的分类任务，其中给出了一组标记点，并希望使用它们对一些未标记的点进行分类。

想象一下，我们有一些数据，如下图所示：

![](../img/51f1a8f087946ee12cc6dd76087f53e4.png)

这里我们拥有二维数据：也就是说，每个点都有两个特征，由平面上的点的`(x,y)`位置表示。另外，我们每个点都有两个类标签之一，这里用点的颜色表示。根据这些功能和标签，我们想创建一个模型，让我们来决定，一个新的点应该被标记为“蓝色”还是“红色”。

许多可能的模型可以用于这样的分类任务，但是在这里我们将使用一个非常简单的模型。我们将假设，通过绘制直线，穿过它们之间的平面，两个分组可以分隔，使得线的每一侧的点落在同一组中。这里的模型是语句“分隔分类的直线”的定量版本，而模型参数是特定数字，描述我们数据的该行的位置和方向。这些模型参数的最优值从数据中学习（这是机器学习中的“学习”），这通常称为训练模型。

下图显示了该数据的训练模型的可视化表示：

![](../img/eecc6395c0d144ea9704c17c33fb486e.png)

现在这个模型已经训练完毕，它可以推广到新的，未标记的数据。 换句话说，我们可以拿一组新的数据，通过它绘制这个模型的直线，并根据这个模型为新的点分配标签。 这个阶段通常称为预测。 见下图：

![](../img/0eadbb216c7a3f8c740bcb07247c345b.png)

这是机器学习中分类任务的基本思想，其中“分类”表示数据具有离散的分类标签。乍一看，这可能看起来相当微不足道：简单地看看这些数据，并画出这样的分隔线来完成这个分类，是比较容易的。然而，机器学习方法的好处是，它可以在更多维度上推广到更大的数据集。

例如，这类似于垃圾邮件自动检测任务；在这种情况下，我们可能会使用以下特征和标签：

*   特征 1，特征 2，以及其它。–> 重要单词和短语的正则化数量 （”Viagra”，”Nigerian prince”，以及其它。）
*   标签 –> “spam”（垃圾）或者 “not spam”（不是垃圾）

对于训练集，这些标签可能通过小型代表性电子邮件样本的个别检查来确定。对于剩余的电子邮件，标签将使用模型确定。对于使用足够良好构造的特征（通常为数千或数百万个单词或短语）来适当训练的分类算法，这种类型的方法可以是非常有效的。我们将在[朴素贝叶斯分类](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.05-Naive-Bayes.ipynb)中看到这种基于文本的分类的例子。

我们将更详细地讨论的，一些重要的分类算法，是高斯朴素贝叶斯（见[朴素贝叶斯分类](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.05-Naive-Bayes.ipynb)），支持向量机（参见[支持向量机](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb)）和随机森林分类（参见[决策树和随机森林](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.08-Random-Forests.ipynb)）。

### 回归：预测连续标签

与分类算法的离散标签相反，我们接下来看一个简单的回归任务，其中标签是连续数量。

考虑下图所示的数据，其中包含一组点，每个点都有一个连续的标签：

![](../img/bd11ac75da04e88d21bee065b5da882e.png)

与分类示例一样，我们拥有二维数据：即，两个特征描述每个数据点。每个点的颜色表示该点的连续标签。

许多可能的回归模型可以用于这种类型的数据，但是在这里我们将使用简单的线性回归来预测点数。 这个简单的线性回归模型假定，如果我们将标签视为第三个空间维度，我们可以将数据拟合为平面。 这是二维数据的线性拟合问题的高阶推广。

![](../img/de1985d2c67ee492d653498ac53317ee.png)

请注意，这里的特征 1 和 2 的平面与之前的二维图相同；然而，在这种情况下，我们已经通过颜色和 Z 轴位置来表示标签。 从这个观点来看，似乎合理的是，这种通过三维数据的拟合平面，允许我们预测任何一组输入参数的预期标签。 返回到二维投影，当我们拟合这样的平面时，我们得到如下图所示的结果：

![](../img/ba04e3f450c1a11f920df6f6525ab698.png)

这个拟合平面告诉了我们，我们需要什么来预测新的点的标签。视觉上，我们在这幅图中，找到了展示的结果：

![](../img/a8050e1b8da9b55ae839fe2b58815b52.png)

与分类示例一样，在数量很少的维度上，这似乎是微不足道的。 但是这些方法的强大之处在于，在数据具有许多特征的的情况下，它们可以被直接应用和评估。

例如，这类似于计算通过望远镜观察到的星系距离的任务 - 在这种情况下，我们可能会使用以下特征和标签：

*   特征 1，特征 2，以及其它 –> 每个星系的亮度，位于几个波长或颜色之一

*   标签：星系的距离或者红移

这几个星系的距离可以通过一组独立的（通常更昂贵的）观测来确定。 然后可以使用合适的回归模型估计剩余星系的距离，而不需要在整个集合中使用更昂贵的观测。 在天文界，这被称为“测光红移”问题。

我们将讨论的一些重要的回归算法是线性回归（参见[线性回归](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.06-Linear-Regression.ipynb)），支持向量机（参见[支持向量机](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb)）和随机森林回归（参见[决策树和随机森林](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.08-Random-Forests.ipynb)）。

### 聚类：在未标记的数据上推断标签

我们刚刚看过的分类和回归图解，是监督学习算法的例子，其中我们正在尝试构建一个模型，预测新数据的标签。 无监督的学习涉及不参照任何已知标签来描述数据的模型。

无监督学习的一个常见情况是“聚类”，其中数据被自动分配给一些数量的离散分组。 例如，我们可能有一些二维数据，如下图所示：

![](../img/8d47bff2360cfeceeb9e7c90d93a1200.png)

通过眼睛，很明显，这些点中的每一个都是不同组的一部分。 给定该输入，聚类模型将使用数据的内在结构来确定哪些点是相关的。 使用非常快速和直观的 k-means 算法（参见 [K 均值](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb)聚类），我们发现如下图所示的簇：

![](../img/932aec8d4877b33a1f6c6c2d77a3fb0b.png)

k-means 适合由 k 个簇中心组成的模型; 假设最优中心是将每个点与其指定中心的距离最小化的中心。 再次，这可能看起来像是二维的微不足道的练习，但随着数据变得越来越复杂，可以使用这种聚类算法从数据集中提取有用的信息。

我们将在 [K-Means 聚类](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb)中更深入地讨论 k-means 算法。 其他重要的聚类算法包括高斯混合模型（参见[高斯混合模型](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb)）和谱聚类（参见 [Scikit-Learn 的聚类文档](http://scikit-learn.org/stable/modules/clustering.html)）。

### 降维：推断未标记数据的结构

降维是无监督算法的另一个例子，其中标签或其他信息是从数据集本身的结构推断的。 降维比我们以前看到的示例有点更加抽象，但通常它试图提取一些数据的低维表示，以某种方式保留完整数据集的相关质量。 不同的降维以不同的方式测量这些相关的质量，我们将在[流形学习](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.10-Manifold-Learning.ipynb)中看到。

作为一个例子，请考虑下图所示的数据：

![](../img/30d8f3132a8fa4d55a7d76f49590d95d.png)

视觉上，这个数据中有一些结构是清楚的：它是从一维直线上绘制的，这个直线在这个二维空间内是螺旋排列的。 在某种意义上，您可以说这个数据是“本质上”是一维的，尽管这个一维数据嵌入在更高维的空间中。 在这种情况下，合适的降维模型对这种非线性嵌入式结构是敏感的，并能够拉取出这种低维度表示。

下图显示了 Isomap 算法的结果的可视化，Isomap 算法是一个专门为此的流形学习算法：

![](../img/c703828202bcb1ff1ac26444ff52d60f.png)

请注意，颜色（表示提取的一维潜在变量）沿着螺旋线均匀变化，这表明该算法实际上检测到了我们眼睛看到的结构。 与前面的例子一样，在更高维度的情况下，降维算法的力量变得更加清晰。 例如，我们可能希望可视化具有 100 或 1,000 个特征的数据集中的重要关系。 可视化 1,000 维数据是一个挑战，一种方法，我们可以使它更易于管理，是使用降维技术将数据减少到二维或三维。

我们将讨论的一些重要的降维算法是主成分分析（参见[主成分分析](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb)）以及各种流形学习算法，包括 Isomap 和局部线性嵌入（参见[流形学习](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.10-Manifold-Learning.ipynb)）。

## 总结

在这里，我们看到了一些机器学习方法的基本类型的简单例子。不用说，我们已经看到了一些重要的实际细节，但是我希望这一节足以给你一个基本的想法，关于机器学习方法可以解决什么类型的问题。

总之，我们看到这些：

监督学习：可以根据标记的训练数据预测标签的模型

*   分类：将标签预测为两个或多个离散类别的模型
*   回归：预测连续标签的模型

无监督学习：识别未标记数据中的结构的模型

*   聚类：在数据中检测和识别不同分组的模型
*   降维：在较高维数据中检测和识别低维结构的模型

在以下部分中，我们将进一步深入分析这些类别，并且可以看到一些更有趣的例子，说明这些概念用于哪些方面。

上述讨论中的所有数据都是基于实际的机器学习计算而生成的；后面的代码可以在[附录：图形代码](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/06.00-Figure-Code.ipynb)中找到。