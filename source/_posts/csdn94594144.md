---
title: "K近邻(kNN)"
id: csdn94594144
---

## K近邻(kNN)简介

> k近邻方法是一种惰性学习算法，可以用于回归和分类，它的主要思想是投票机制，对于一个测试实例 x j x_j xj​, 我们在有标签的训练数据集上找到和最相近的k个数据，用他们的label进行投票，分类问题则进行表决投票，回归问题使用加权平均或者直接平均的方法。

##### 整体介绍

正所谓物以类聚,人以群分,kNN就是利用这个思想的一种学习算法, 对于每一个预测的实例,找打和它相近的k个实例,用这k个实例的平均水平表示这个待预测的实例. 如果是一个预测一个人平均收入的问题,我们都知道只要知道他的k个朋友的相关收入求平均值即可. KNN就是这么做的,只是需要指定K,并且怎么判断"朋友".

##### 伪代码:

输入:训练数据 T = ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x N , y n ) T={(x_1,y_1),(x_2,y_2),...,(x_N,y_n)} T=(x1​,y1​),(x2​,y2​),...,(xN​,yn​) 其中 x i ∈ R n x_i \in R^n xi​∈Rn,是实例的特征向量， y i ∈ Y = c 1 , c 2 , . . . , c K y_i \in Y = {c_1,c_2,...,c_K} yi​∈Y=c1​,c2​,...,cK​,表示类别

输出: 实例x所属的类别
根据跟定的距离度量的方法，在T中找到和x最邻近的k个点，记作x的邻域， N k ( x ) N_k(x) Nk​(x)
在 N k ( x ) N_k(x) Nk​(x)中使用多数表决规则，绝对x的类别y: y = a r g m a x c j ∑ x i ∈ N k ( x ) I ( y i = c j ) y=argmax_{c_j} \sum_{x_i \in N_k(x)}I(y_i=c_j) y=argmaxcj​​xi​∈Nk​(x)∑​I(yi​=cj​)
对于回归问题,得到y y = 1 k ∑ x i ∈ N k ( x ) y i y= \frac{1}{k} \sum_{x_i \in N_k(x)}y_i y=k1​xi​∈Nk​(x)∑​yi​ 其中 i = 1 , 2 , . . . , N ; j = 1 , 2 , . . . , K i=1,2,...,N; j=1,2,...,K i=1,2,...,N;j=1,2,...,K.

## 核心公式

1.  距离的度量(怎么判断朋友)

> 对于两个向量 ( x i , x j ) (x_i, x_j) (xi​,xj​),一般使用 L p L_p Lp​距离进行计算。 假设特征空间 X X X是n维实数向量空间 R n R^n Rn, 其中， x i , x j ∈ X x_i,x_j \in X xi​,xj​∈X, x i = ( x i ( 1 ) , x i ( 2 ) , . . . , x i ( n ) ) ， x j = ( x j ( 1 ) , x j ( 2 ) , . . . , x j ( n ) ) x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})，x_j=(x_j^{(1)},x_j^{(2)},...,x_j^{(n)}) xi​=(xi(1)​,xi(2)​,...,xi(n)​)，xj​=(xj(1)​,xj(2)​,...,xj(n)​)，
> x i , x j x_i,x_j xi​,xj​的 L p L_p Lp​距离定义为:
> L p ( x i , x j ) = ( ∑ l = 1 n ∣ x i ( l ) − x j ( l ) ∣ p ) 1 p L_p(x_i,x_j) = \left( \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p \right) ^ {\frac{1}{p}} Lp​(xi​,xj​)=(l=1∑n​∣xi(l)​−xj(l)​∣p)p1​ 这里的 p ≥ 1 p \geq 1 p≥1. 当p=2时候，称为欧氏距离(Euclidean distance), 有 L 2 ( x i , x j ) = ( ∑ l = 1 n ∣ x i ( l ) − x j ( l ) ∣ 2 ) 1 2 L_2(x_i,x_j) = \left( \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^2 \right) ^ {\frac{1}{2}} L2​(xi​,xj​)=(l=1∑n​∣xi(l)​−xj(l)​∣2)21​ 当p=1时候，称为曼哈顿距离(Manhattan distance), 有 L 1 ( x i , x j ) = ∑ l = 1 n ∣ x i ( l ) − x j ( l ) ∣ L_1(x_i,x_j) = \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}| L1​(xi​,xj​)=l=1∑n​∣xi(l)​−xj(l)​∣ 当 p = ∞ p=\infty p=∞时候，称为极大距离(infty distance), 表示各个坐标的距离最大值， 有 L p ( x i , x j ) = max ⁡ l n ∣ x i ( l ) − x j ( l ) ∣ L_p(x_i,x_j) = \max_{l}{n}|x_i^{(l)}-x_j^{(l)}| Lp​(xi​,xj​)=lmax​n∣xi(l)​−xj(l)​∣

2.  k值的选择

> kNN中的k是一个超参数，需要我们进行指定，一般情况下这个k和数据有很大关系，都是交叉验证进行选择，但是建议使用交叉验证的时候， k ∈ [ 2 , 20 ] k \in [2,20] k∈[2,20], 使用交叉验证得到一个很好的k值。
> k值还可以表示我们的模型复杂度，当k值越小意味着模型复杂度表达，更容易过拟合，(用极少树的样例来绝对这个预测的结果，很容易产生偏见，这就是过拟合)。我们有这样一句话，k值越大学习的估计误差越小，但是学习的近似误差就会增大.

3.  怎么理解 “k值越大学习的估计误差越小，但是学习的近似误差就会增大”

> 估计误差表示最后的结果，k值大，集百家所长，更可能得到准确的值，表示估计的准确，则误差就小；但是我们的估计的时候，在学习过程中，使用最相近的k个实例进行估计，每一个值都会和预测的x有一个近似误差，k越大则误差的总和就越大。

4.  多数表决等价于经验风险最小化

> 《统计学习方法》第40页 分类决策规则. 多说一句，既然要求误差，那就写出误差损失函数，剩下的就是公式恒等变化了.

5.  KD树

> kNN，从上述算法中，我们可以看到主要是从训练数据中，知道k个相近实例，但是每次都要便利这个数据集合，主要的问题就是速度慢. 这时候就出现了加速查找的数据结构，其中之一就是kd 树.

> *   构造kd 树
>     kd树是一种对k维空间中的实例店进行存储以便能够进行快速检索的数据结构. kd树是一个二叉树，表示对k维空间的一个划分. 构造kd树就是不断用垂直于坐标轴的超平面 (一般用每一个维度的中位数来表示) 将k维空间划分，构成一系列的k维超矩形区域.

> *   伪代码: (from wikipedia)
>     function kdtree (list of points pointList, int depth)
>     {
>     // Select axis based on depth so that axis cycles through all valid values
>     var int axis := depth mod k;
>     // Sort point list and choose median as pivot element
>     select median by axis from pointList;
>     // Create node and construct subtree
>     node.location := median;
>     node.leftChild := kdtree(points in pointList before median, depth+1);
>     node.rightChild := kdtree(points in pointList after median, depth+1);
>     return node;
>     }

> *   基于kd树查找最近邻
>     构造kd树的目的就是快速查找最近邻和k近邻，这里我们给出二维的列子，这里例子来自与书中和 wiki百科，我尝试说明百这几张图的运行原理.
>     数据 T = {(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)}
>     构造kd树，二叉树和空间角度的划分图，再次注意每一次用的这个维度对应的所有这个空间中的中位数.
>     ![在这里插入图片描述](../img/aec1377533c4d67e267c0a551f1b07d9.png)

> *   二叉树划分图
>     根据x维度有{2,4,5,7,8,9}: 中位数是7，因此(7,2)作为根节点，x < 7,在左子树，其他在右子树。依次递归构造左右子树，下一次根据维度y，之后根据维度x. 注意这里的k是维度，n 可能大学k，故要每次对k取余数. （这个k不是kNN中的k）.
>     下面的空间划分图，表示空间上的显示格式. 其实整体的搜索是在这个空间上进行的.
>     ![在这里插入图片描述](../img/977e7dba52dfc87bb15784c95da4738d.png)

> *   空间划分图
>     下面用一个gif图来表示搜索最近邻的过程,简单来说就是一句话，根据构造过程，从头到尾左右二分，在这个过程中记录下来最近的点。从这个图中，我们可以看到，我们目标实例target用四角星表示
>     ![在这里插入图片描述](../img/ebf600b24040869be444b85f00324868.png)
> 
> 1.  二叉树搜索，依次到叶子节点D，路劲是A - B - D. 这个过程中最近的点是B，作为最有候选集合.
> 2.  我们以target为圆心，target到B的距离为半径，画圆，我们发现，B的另外的部分与圆相交，这表示，可能存在更近的候选点在另半部分，如果不存在相交，此时的候选点B就是最近点。
> 3.  如果存在和其他空间相交，则将搜索空间上升为其父节点，用target和其父节点距离作圆，如果依然和其他空间相交，继续回溯搜索，直到不想交或者全部搜索完毕找到候选点.
> 4.  上面介绍的是最近邻查找，如何查找k近邻？
> 
> > 使用各最大堆数据结果，维护k大小的最大堆，从根节点开始如果堆的大小不足k，就候选集如果
> > 如果大小为k，就比较堆顶元素和当前元素的距离大小，如果当前小于堆顶距离就进行替换，
> > 之后以堆顶元素为中心，就编程了找最近邻问题。最后返回结果

## 算法十问

1.  简述一下KNN算法的原理

> KNN算法利用训练数据集对特征向量空间进行划分。KNN算法的核心思想是在一个含未知样本的空间，可以根据样本最近的k个样本的数据类型来确定未知样本的数据类型。 该算法涉及的3个主要因素是：k值选择，距离度量，分类决策。

2.  如何理解kNN中的k的取值？

> 在应用中，k值一般取比较小的值，并采用交叉验证法进行调优。

3.  在kNN的样本搜索中，如何进行高效的匹配查找？

> 线性扫描(数据多时，效率低) 构建数据索引——Clipping和Overlapping两种。前者划分的空间没有重叠，如k-d树；后者划分的空间相互交叠，如R树。（对R树了解很少，可以之后再去了解）

4.  KNN算法有哪些优点和缺点？

> 优点：算法思想较简单，既可以做分类也可以做回归；可以用于非线性分类/回归；训练时间复杂度为O(n)；准确率高，对数据没有假设，对离群点不敏感。
> 缺点：计算量大；存在类别不平衡问题；需要大量的内存，空间复杂度高。

5.  不平衡的样本可以给KNN的预测结果造成哪些问题，有没有什么好的解决方式？

> 输入实例的K邻近点中，大数量类别的点会比较多，但其实可能都离实例较远，这样会影响最后的分类。
> 可以使用权值来改进，距实例较近的点赋予较高的权值，较远的赋予较低的权值。

6.  为了解决KNN算法计算量过大的问题，可以使用分组的方式进行计算，简述一下该方式的原理。

> 先将样本按距离分解成组，获得质心，然后计算未知样本到各质心的距离，选出距离最近的一组或几组，再在这些组内引用KNN。
> 本质上就是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本，该方法比较适用于样本容量比较大时的情况。

## 面试真题

1.  KD树？怎么构建的？

> kd树是对数据点在k维空间中划分的一种数据结构，主要用于多维空间关键数据的搜索。本质上，kd树就是一种平衡二叉树。

2.  K-Means与KNN有什么区别

> *   KNN
>     1.KNN是分类算法
>     2.监督学习
>     3.喂给它的数据集是带label的数据，已经是完全正确的数据
>     4.没有明显的前期训练过程，属于memory-based learning
>     5.K的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c
> *   K-Means
>     1.K-Means是聚类算法
>     2.非监督学习
>     3.喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序
>     4.有明显的前期训练过程
>     5.K的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识
> *   相似点：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法，一般用KD树来实现NN。

3.  KD树改进

> *   Kd-tree在维度较小时（例如：K≤30），算法的查找效率很高，然而当Kd-tree用于对高维数据（例如：K≥100）进行索引和查找时，就面临着维数灾难（curse of dimension）问题，查找效率会随着维度的增加而迅速下降。通常，实际应用中，我们常常处理的数据都具有高维的特点，例如在图像检索和识别中，每张图像通常用一个几百维的向量来表示，每个特征点的局部特征用一个高维向量来表征（例如：128维的SIFT特征）。因此，为了能够让Kd-tree满足对高维数据的索引，Jeffrey S. Beis和David G. Lowe提出了一种改进算法——Kd-tree with BBF（Best Bin First），该算法能够实现近似K近邻的快速搜索，在保证一定查找精度的前提下使得查找速度较快。
> *   在介绍BBF算法前，我们先来看一下原始Kd-tree是为什么在低维空间中有效而到了高维空间后查找效率就会下降。在原始kd-tree的最近邻查找算法中（第一节中介绍的算法），为了能够找到查询点Q在数据集合中的最近邻点，有一个重要的操作步骤：回溯，该步骤是在未被访问过的且与Q的超球面相交的子树分支中查找可能存在的最近邻点。随着维度K的增大，与Q的超球面相交的超矩形（子树分支所在的区域）就会增加，这就意味着需要回溯判断的树分支就会更多，从而算法的查找效率便会下降很大。
> *   从上述标准的kd树查询过程可以看出其搜索过程中的“回溯”是由“查询路径”决定的，并没有考虑查询路径上一些数据点本身的一些性质。一个简单的改进思路就是将“查询路径”上的结点进行排序，如按各自分割超平面（也称bin）与查询点的距离排序，也就是说，回溯检查总是从优先级最高（Best Bin）的树结点开始。

> *   bbf的算法:
>     输入：kd树，查找点x.
>     输出：kd树种距离查找点最近的点以及最近的距离
> 
> 1.  若kd树为空，则设定两者距离为无穷大，返回；如果kd树非空，则将kd树的根节点加入到优先级队列中；
> 2.  从优先级队列中出队当前优先级最大的结点，计算当前的该点到查找点的距离是否比最近邻距离小，如果是则更新最近邻点和最近邻距离。如果查找点在切分维坐标小于当前点的切分维坐标，则把他的右孩子加入到队列中，同时检索它的左孩子，否则就把他的左孩子加入到队列中，同时检索它的右孩子。这样一直重复检索，并加入队列，直到检索到叶子节点。然后在从优先级队列中出队优先级最大的结点；
> 3.  重复1和1中的操作，直到优先级队列为空，或者超出规定的时间，返回当前的最近邻结点和距离。

## 参考

1.  [https://blog.csdn.net/weixin_44915167/article/details/89315734](https://blog.csdn.net/weixin_44915167/article/details/89315734)
2.  [https://www.cnblogs.com/nucdy/p/6349172.html](https://www.cnblogs.com/nucdy/p/6349172.html)
3.  [https://blog.csdn.net/v_july_v/article/details/8203674](https://blog.csdn.net/v_july_v/article/details/8203674)
4.  [https://blog.csdn.net/junshen1314/article/details/51121582](https://blog.csdn.net/junshen1314/article/details/51121582)
5.  [https://blog.csdn.net/lhanchao/article/details/52535694](https://blog.csdn.net/lhanchao/article/details/52535694)
6.  [https://blog.csdn.net/fool_ran/article/details/85246432](https://blog.csdn.net/fool_ran/article/details/85246432)
7.  李航 统计学习方法