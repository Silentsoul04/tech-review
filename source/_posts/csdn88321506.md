---
title: PyTorch 1.0 中文文档：torch.autograd
id: csdn88321506
---

> 译者：[gfjiangly](https://github.com/gfjiangly)

`torch.autograd` 提供类和函数，实现任意标量值函数的自动微分。 它要求对已有代码的最小改变—你仅需要用`requires_grad=True`关键字为需要计算梯度的声明`Tensor`。

```
torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None) 
```

计算被给张量关于图的叶节点的梯度和。

图使用链式法则微分。如何任何`tensors`是非标量（例如他们的数据不止一个元素）并且要求梯度，函数要额外指出`grad_tensors`。它应是一个匹配长度的序列，包含可微函数关于相应张量的梯度（`None`是一个对所有张量可接受的值，不需要梯度张量）。

此函数在叶节点累积梯度 - 你可能需要在调用前把它初始化为0.

参数：

*   **tensors** (*Tensor序列*) – 计算导数的张量。
*   **grad_tensors** (*[*Tensor*](tensors.html#torch.Tensor "torch.Tensor") *或* [*None*](https://docs.python.org/3/library/constants.html#None "(in Python v3.7)")序列*) – 关于相应张量每个元素的梯度。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。
*   **retain_graph** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.7)")*,* *可选*) – 如果False，用于计算梯度的图将被释放。请注意，在几乎所有情况下，不需要将此选项设置为真，而且通常可以更有效地解决问题。默认为create_graph值。
*   **create_graph** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.7)")*,* *可选*) – 如果True，则构造导数图，以便计算更高阶导数，默认False。

```
torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False) 
```

计算和返回输出关于输入的梯度和。

`grad_outputs` 应是长度匹配输出的序列，包含关于输出每个元素的预计算梯度。如果一个输出不要求梯度，则梯度是`None`。

如果`only_inputs`是`True`，此函数将仅返回关于指定输入的梯度list。如果此参数是`False`，则关于其余全部叶子的梯度仍被计算，并且将累加到`.grad`属性中。

> [**阅读全文／改进本文**](https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.0/autograd.md)