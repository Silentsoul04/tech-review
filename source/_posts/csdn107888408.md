---
title: 机器学习中的优化算法！
id: csdn107888408
---

↑↑↑关注后"星标"Datawhale

每日干货 & [每月组队学习](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIyNjM2MzQyNg%3D%3D&action=getalbum&album_id=1338040906536108033#wechat_redirect)，不错过

 Datawhale干货 

**作者：李祖贤，****Datawhale高校群成员****，深圳大学**

在机器学习中，有很多的问题并没有解析形式的解，或者有解析形式的解但是计算量很大（譬如，超定问题的最小二乘解），对于此类问题，通常我们会选择采用一种迭代的优化方式进行求解。

负梯度方法与Newton型方法在最优化方法中发挥着重要作用，也在现代金融科技，大规模的机器学习发挥不可或缺的作用。接下来，我们将针对这两种优化方法在机器学习中的应用进行讨论。

**一、****最速下降法**

**1.1 最速下降法的原理**

假定在第k步的迭代点![](../img/9200aaaf3ad24409c274bdb85e308ed0.png)，我们想求![](../img/9200aaaf3ad24409c274bdb85e308ed0.png)处使得 ![](../img/f970fc584364467896d0e2fc748040b9.png) 下降最快的方向。由上一章可知：这个方向应首先满足下降条件![](../img/da0d699aeb140e2a8971735c8e512ac0.png)。虽然下降方向有无穷多个，但是根据Cauchy-Schwarz不等式：![](../img/d652c67c6a0ab400d6b883c302f410d1.png)当且仅当![](../img/18d634560fa6de74d0121de30d357910.png)时等式成立，![](../img/43aff2c9450fecd8a54c46e38e01b29e.png)达到最小。由于在![](../img/aa46ccc2d18ccdd5f5c56f21f362bbc4.png)方向上要考虑步长，故取![](../img/aa46ccc2d18ccdd5f5c56f21f362bbc4.png)为负梯度方向：![](../img/272168ee1aacfb6c6fc5c863ebda05d9.png)。

特别的，我们称采用负梯度方向以及精确线搜索的方法称为最速下降法。

![](../img/0542c37d60973c2560dd8930f2ddec97.png)

![](../img/a27bcda93279ea64ea600c885aa33e46.png) ![](../img/1220b65bbe43a836b94f310338dfc072.png)

我们从上面可以看到，不同的G矩阵使用最速下降法的迭代速度有明显的差异，原因在后文给出。

## **1.2 最速下降法的收敛速度**

**1.2.1  收敛性**

最速下降法具有全局收敛性！

**1.2.2  预备知识**

*   向量u在矩阵G度量下的范数：

*   矩阵G度量下的Cauchy-Schwarz不等式：

![](../img/0728b5e18f4d59281b2601098264da41.png)

*   Kantorovich不等式：

![](../img/b73cef04f8da41af80723f60c77125b5.png)

**1.2.3  收敛速度的上界**

正定二次函数： ![](../img/aae31089c28dc8e01978d314d5d1a7e4.png)

收敛速度的上界:

![](../img/4e2ab49124bdcdcf2875a35b42175e47.png)

由此可知，最速下降法的收敛速度是线性的，这个速度依赖于G的最大最小特征值。

**1.2.4  收敛速度的差异性来源**

我们假设G和b产生了微小扰动变成了![](../img/5e9ebfd057678c2fe1daef9719eea97e.png) ,正定二次函数： ![](../img/aae31089c28dc8e01978d314d5d1a7e4.png)的导函数方程相应变成了 ![](../img/4ec4ea2b3b4f83499f063f4cf418fa76.png) ，方程的解记为![](../img/5ea84454218f3d74695f21fe7621d30d.png) ,其中![](../img/68379a14c1ac7f01934b460c74c55613.png)非奇异，![](../img/b774600829f31d127dad2f1774e28432.png) 满足![](../img/74bd20620fe5d27240e747b2adbc2d39.png) 非零。那么：

![](../img/d1a1928e1195edce99b29e77a6678126.png)

条件数与范数有关，因此是G的相对误差与b的相对误差之和的放大倍数。若矩阵G的条件数很大，扰动对解的影响很大，我们称这个问题是病态的，或G是病态的。若矩阵G的条件数不大，扰动对解的影响程度不大，我们就成这样的问题是良性的，或G是良性的。

因此：

![](../img/50d569f5b03d077cf0dd3306a99baddb.png)

这说明最速下降法的收敛速度依赖G的条件数，当G的条件数接近于1时，![](../img/55a79bfb0f327ac91db35c282d531eb9.png)接近于0，最速下降法的收敛速度接近于超线性收敛；而当G的条件数很大时，![](../img/55a79bfb0f327ac91db35c282d531eb9.png)接近于1，则收敛很慢。

![](../img/2eba8d76e0b55df7b748e6fa473be6c7.png) ![](../img/1223f4a48096df3c396ca4c7bc4cf3cf.png)

**1.2.5  最速下降法的优缺点**

**优点**：算法每次迭代的计算量少，储存量也少，从一个不太好的初始点出发也能靠近极小点。

**缺点**：

*   收敛慢：线性收敛。

*   Zigzag现象（收敛慢的原因）：若迭代步 ![](../img/88ac84998899b2f51c3f159fb8cdef62.png) 是 ![](../img/87320eca5b6e66c78453ad0ed03740d8.png) 的精确最小点，则![](../img/739d4ab350e0de2554fbffeeb66a5533.png)，因此： ![](../img/7be9f484c5073d8cce846addbde76965.png) ，也就是上一步的方向与下一步的方向垂直。

![](../img/42a5c8da0e632e80e89c947cb9504a56.png)

*   没有二次终止性：即不具备对于任意的正定二次函数，从任意点出发，都可以经过有限步迭代取得极小值的性质。

## **二、Newton方法**

**2.1 基本Newton方法**

设![](../img/f970fc584364467896d0e2fc748040b9.png) 具有连续二阶偏导数，当前迭代点是![](../img/9200aaaf3ad24409c274bdb85e308ed0.png) 。![](../img/f970fc584364467896d0e2fc748040b9.png) 在 ![](../img/9200aaaf3ad24409c274bdb85e308ed0.png) 的泰勒展开为：

 ![](../img/8ae95bb5d7737b4394333ae7b7c7dedd.png) 

其中![](../img/40c057ff05e5e8ab8098cbba4f5b0568.png)。在点![](../img/9200aaaf3ad24409c274bdb85e308ed0.png)的邻域内，用二次函数![](../img/1bc4e4d06c5efaf8bd4e499db626e3cb.png)去近似![](../img/36cc18093a3728b7a3efd8069bf33b7a.png)，求解问题![](../img/8f7e3ba12345b6536296f1026758f882.png) 。

若![](../img/c750c8038a68bbb2b5bdc3ab968f7096.png)正定，则迭代方向![](../img/910c01b06f1eb6cb1cceff1d0d2947d5.png)为问题的唯一解。我们称![](../img/910c01b06f1eb6cb1cceff1d0d2947d5.png)为Newton方向。(Hesse的逆矩阵度量下的最速下降法)

![](../img/86b4c1cba6e538141da60ee873933c1b.png)

我们来看看牛顿迭代的方向和梯度下降的方向有什么不一样？（黑色为牛顿下降方向，红色为负梯度下降方向）

![](../img/70408b9c12878e7b4be508d883194ed0.png)

下面我们用一个具体的例子来看看牛顿迭代法的效果：

![](../img/569691f7df38f1aad905728382d39e5f.png) ![](../img/ae1475f683f4371c683ca1e478cd9ea7.png)

从上面的例子我们可以看到：

（1）当初始点接近极小点时，迭代序列收敛于极小点，并且收敛很快（二阶收敛）；

（2）当初始点不接近极小点时，迭代序列容易收敛到鞍点或者极大点（局部收敛性而不是全局收敛）。

（3）迭代过程可能会出现奇异矩阵或者病态，以至于求逆很困难，导致迭代失败。

*   当![](../img/b5ecc1b69eed5f74ad808d972766c165.png)的特征值![](../img/7d3e7ee3f0da0a6121e4d82c1d83d944.png)，![](../img/665e6ad3044564619195cd32b9f6e202.png)求不出来。

*   当![](../img/b5ecc1b69eed5f74ad808d972766c165.png)的特征值

    ![](../img/74fea3c4b31f576fd15c753437f849d0.png) 

    不一定小于0，牛顿方向未必是下降方向。

（4）每一步迭代需要计算Hesse矩阵，即计算n(n+1)/2个二阶偏导数，相当于求解一个线性方程组，计算量为O(![](../img/4a8d036bb91cec19235497b6950117d7.png))

## **2.2 阻尼Newton方法**

为了改善基本Newton方法的局部收敛准则，我们采用带一维线搜索的的Newton方法，即

![](../img/9a20cce1dede3dd1881449031d4d66ee.png)

其中![](../img/88ac84998899b2f51c3f159fb8cdef62.png)是一维搜索的结果，该方法叫做阻尼Newton方法。此方法能保证对正定矩阵![](../img/c750c8038a68bbb2b5bdc3ab968f7096.png)，![](../img/a8ad88b61aa4a441635fdb25cde9a7f4.png) 单调下降；即使 ![](../img/9200aaaf3ad24409c274bdb85e308ed0.png) 离x稍远，由该方法产生的点列![](../img/07cc040b635b858767cb87374dcea346.png)仍能收敛到![](../img/409ca8e897e1003924e9347cd1fa09ac.png)。（对严格凸函数具有全局收敛性）

## **2.3 混合方法**

基本Newton方法在迭代过程中会出现Hesse矩阵奇异、不正定的情形，基本Newton方法还会出现与![](../img/fd684c2191a057e2ba7a4e3acace0aeb.png)几乎正交的情形。为了解决这个问题，我们可以采用基本Newton方法与最速下降法相互混合的方式。

该方法采用Newton方法，但是在Hesse矩阵![](../img/c750c8038a68bbb2b5bdc3ab968f7096.png)奇异或者![](../img/fd684c2191a057e2ba7a4e3acace0aeb.png)与![](../img/aa46ccc2d18ccdd5f5c56f21f362bbc4.png)几乎正交时，采用负梯度方向；在![](../img/c750c8038a68bbb2b5bdc3ab968f7096.png)负定，但是![](../img/23fb9e931d7a660cc98bedbfdacef190.png)存在时，取![](../img/4e911e4067cab108fb7e058d31b91f30.png) 。

![](../img/247b313081a3d27a076a805b90b40796.png)

## **2.4 LM方法**

LM方法是处理![](../img/c750c8038a68bbb2b5bdc3ab968f7096.png)奇异、不正定等情况的一个最简单有效的方法，它是指求解 ![](../img/57c9059049bab8c7d1cff5b88552fc59.png)来确定迭代方向的Newton型方法，这里的![](../img/225dfd4fdda11ac1752e1ed1c36a4843.png)是单位阵。显然，若![](../img/e8f6cd9dcd236d8eb3d36f6306d7a2f6.png)足够大，可以保证![](../img/71cb3a514f9a9269aa08e2b49e61a771.png)正定。

（1） ![](../img/e8f6cd9dcd236d8eb3d36f6306d7a2f6.png) 的大小对于方向的影响：

*   当 ![](../img/e8f6cd9dcd236d8eb3d36f6306d7a2f6.png) 很小，求出的步长偏向于Newton方向。

*   当 ![](../img/e8f6cd9dcd236d8eb3d36f6306d7a2f6.png) 很大，求出的步长则偏向于负梯度方向。

（2）当![](../img/71cb3a514f9a9269aa08e2b49e61a771.png)不正定时，可以简单取![](../img/90a6c4969a73cfb1ffe13fb0d0a1f08f.png)

![](../img/5075fadfd3abf1ded9c71f5eaa37da01.png) ![](../img/dd7e4fb2d64fd4270db092d852df94e1.png) ![](../img/0dd43f7d578b5e9c4a5eb673fad53ccd.png) ![](../img/b9101d37b27c584b6264e9b044be072e.png)

## **三、拟牛顿方法**

Newton方法的优缺点：

（1）当初始点接近极小点时，迭代序列收敛于极小点，并且收敛很快（二阶收敛）；

（2）当初始点不接近极小点时，迭代序列容易收敛到鞍点或者极大点（局部收敛性而不是全局收敛）。

（3）迭代过程可能会出现奇异矩阵或者病态，以至于求逆很困难，导致迭代失败。

*   当![](../img/b5ecc1b69eed5f74ad808d972766c165.png)的特征值![](../img/7d3e7ee3f0da0a6121e4d82c1d83d944.png)，![](../img/665e6ad3044564619195cd32b9f6e202.png)求不出来。

*   当![](../img/b5ecc1b69eed5f74ad808d972766c165.png)的特征值![](../img/79d2a898313e3f03883bf5d4fac16072.png), 

    ![](../img/a1e0f0645ab15d18b0315071b0a488f4.png)

    不一定小于0，牛顿方向未必是下降方向。

（4）每一步迭代需要计算Hesse矩阵，即计算n(n+1)/2个二阶偏导数，相当于求解一个线性方程组，计算量为O(![](../img/4a8d036bb91cec19235497b6950117d7.png))

为此，我们考虑构造一种方法，她既不需要计算二阶偏导数，又有较快的收敛速度。

## **3.1 拟牛顿条件**

假定当前迭代点为![](../img/43a3b6f1184eb779f71ed2c411995bfa.png)，已知条件为![](../img/a9fed2157fc05c51726c9b837b1928d8.png)，我们使用拉格朗日中值定理： 

![](../img/eead8680a627189fbebf86c81d796800.png)

我们可以使用矩阵![](../img/032fb829718df91134b30a84ead694d3.png)似![](../img/cce23ada66280033336f1e5b9ce9c3dc.png)得到 ![](../img/525e47c06160b60a109659911c5b7da0.png) n个方程，n(n+1)/2个变量。

令![](../img/9d23673f821a1ea2867b54df8b7bbc01.png)得到：

 ![](../img/a6adfd8cf1a4fe0a4a7951c126a324ea.png)

因此拟牛顿条件为：

 ![](../img/a6adfd8cf1a4fe0a4a7951c126a324ea.png) 

满足这两个方程的矩阵有很多，因此拟牛顿方法是一类方法。

![](../img/c702f7f410a3df89c816b4ae80a2720e.png)

在上述算法中，初始矩阵![](../img/3441a0a741b8ddd4db97657696c8cea9.png)一般取单位矩阵，第一步迭代方向取为负梯度方向。

那么，算法的核心就是怎么由![](../img/3752738153957621dc3e5e85baf7912c.png)去修正![](../img/d468b8ec84242d051df1c758b47bf3a2.png)，即![](../img/45ba4854409a464f8d048682630b3a66.png)，而![](../img/50b7017c7dbc0d2ff33c548b10142362.png)的取法是多种多样的，但是他应该具有简单、计算量小、有效的特点。

## **3.2 拟牛顿方法的修正公式**

## **3.2.1 对称秩1公式**

即取![](../img/50b7017c7dbc0d2ff33c548b10142362.png)为对称秩1矩阵，即有

![](../img/0adf2aacc7023d498187fe23430fe4a3.png)。

将![](../img/6b02d4b55c83ad792bde758da4d561ee.png)代入拟牛顿方程 ![](../img/aaf1b4530cb7cc4cd7dc400acbb86032.png) 得到： 

![](../img/e7681e888f50e8a9aea3c52134b91e7e.png) 

即有：![](../img/c114c6ed3e348c6d263d5746d9380c46.png) 。

由于![](../img/e5714d6ad3f4c84e299d3a0667a93e45.png)是一个数，因此u与![](../img/8d138277a6e42a07ad47fd45ab590076.png)共线，从而存在![](../img/6a5632827f952a45a81ea4064cb61cd2.png)使得：![](../img/e5ff051f1026377e1909b61771aef65f.png) 。

将![](../img/e5ff051f1026377e1909b61771aef65f.png)代入![](../img/c114c6ed3e348c6d263d5746d9380c46.png)得到

 ![](../img/dcbdfac0e4aa78d661056412c0490bb1.png) 

因此![](../img/d492da6bb104fe650dd896c25fe15b26.png),由此得到

![](../img/8252ff973ccaea1da3fd7ebaf87e89aa.png) .

最终得到对称秩1公式：

![](../img/722ad83ba33b820146659b1f88017298.png)

如果我们想将![](../img/d468b8ec84242d051df1c758b47bf3a2.png)换成等价的![](../img/032fb829718df91134b30a84ead694d3.png)，则需要用到SMW公式：

![](../img/e1ae780c1a0ff305e6922f3f3856cf1f.png)

最终得到对称秩1公式：

![](../img/39ebdf38a68790bdf7c8664ea0f47c86.png)

## **3.2.2 对称秩2公式**

若![](../img/50b7017c7dbc0d2ff33c548b10142362.png)为对称秩2矩阵，即![](../img/4024bbd120e52c3724e7cf5527e2d441.png)，其中 ![](../img/43334f0f8ad142578e205121bb97a54d.png)待定。

将![](../img/4024bbd120e52c3724e7cf5527e2d441.png)代入![](../img/aaf1b4530cb7cc4cd7dc400acbb86032.png)中，得到![](../img/3752738153957621dc3e5e85baf7912c.png)的修正公式![](../img/9886e7ab1f238fdafa25782cee0200b3.png)。

## **（1）DFP方法**

在![](../img/9886e7ab1f238fdafa25782cee0200b3.png)中，化简为

 ![](../img/7fe16d411301c653d15c7033131c59e5.png) 

由于![](../img/1c923770b25e02d3de57f96d1a0a53e9.png)的选择不是唯一的，为了计算方便，我们选择:

![](../img/feed2d810840b1644b25d0b7b50f7c5c.png)

代入公式中可得 ![](../img/aaf1b4530cb7cc4cd7dc400acbb86032.png) ，得到DFP公式：

![](../img/0fad680119aa4cbe8c7ecc15147fc68c.png)

根据SMW公式：

![](../img/2cabb7b6486914d864b5877717bdd52a.png)

## **（2）BFGS公式（对偶）**

考虑![](../img/a5503982f17b5bcc2133cc0ad688b214.png)的修正公式： ![](../img/8ccf8620275640429608d718cf529ab6.png)用相同的推断实现：

![](../img/49c887d9e4dc14f696dc1a62489d8770.png)

根据SMW公式：

![](../img/a8a5de439f7c8bdde8d4799f7fb7e7f0.png)

## **（3）Broyden族公式**

DFP方法与BFGS公式的线性组合：

![](../img/e1be4339f1af6142a0a2361491a5cf03.png)

## **3.3 三种拟牛顿方法的对比试验**

**（1）扩展Rosenbrock问题**

(BFGS与DFP差异不大，SR1差些)（迭代次数与函数调用次数）

![](../img/8dd5777ce5091f15c9600b9245fc96a5.png) ![](../img/1aec3bea213c7a85e85abc79a837f0c1.png)

（2）由人工神经网络解微分方程的问题：

![](../img/91b685d902636cfa8d807ff047aa97c9.png) ![](../img/04812b891ad2daf0baa0d369cdcb13ff.png) ![](../img/85e17e4065e3dfa578676729e2c9ce82.png)

## **四、使用牛顿法优化Rosenbrock函数实例（基于python）**

Rosenbrock函数的数据探索：

 ![](../img/3532111c2b0788633001bc4eeeff8c55.png)

![](../img/c9a96bbfbf1c66d8a095a9ff8dfaa5a9.png)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
%matplotlib inline
from mpl_toolkits.mplot3d import Axes3D
class Rosenbrock():
    def __init__(self):
        self.x1 = np.arange(-100, 100, 0.0001)
        self.x2 = np.arange(-100, 100, 0.0001)
        #self.x1, self.x2 = np.meshgrid(self.x1, self.x2)
        self.a = 1
        self.b = 1
        self.newton_times = 1000
        self.answers = []
        self.min_answer_z = []

    # 准备数据
    def data(self):
        z = np.square(self.a - self.x1) + self.b * np.square(self.x2 - np.square(self.x1))
        #print(z.shape)
        return z

    # 随机牛顿
    def snt(self,x1,x2,z,alpha):
        rand_init = np.random.randint(0,z.shape[0])
        x1_init,x2_init,z_init = x1[rand_init],x2[rand_init],z[rand_init]
        x_0 =np.array([x1_init,x2_init]).reshape((-1,1))
        #print(x_0)

        for i in range(self.newton_times):
            x_i = x_0 - np.matmul(np.linalg.inv(np.array([[12*x2_init**2-4*x2_init+2,-4*x1_init],[-4*x1_init,2]])),np.array([4*x1_init**3-4*x1_init*x2_init+2*x1_init-2,-2*x1_init**2+2*x2_init]).reshape((-1,1)))
            x_0 = x_i
            x1_init = x_0[0,0]
            x2_init = x_0[1,0]
        answer = x_0
        return answer

    # 绘图
    def plot_data(self,min_x1,min_x2,min_z):
        x1 = np.arange(-100, 100, 0.1)
        x2 = np.arange(-100, 100, 0.1)
        x1, x2 = np.meshgrid(x1, x2)
        a = 1
        b = 1
        z = np.square(a - x1) + b * np.square(x2 - np.square(x1))
        fig4 = plt.figure()
        ax4 = plt.axes(projection='3d')
        ax4.plot_surface(x1, x2, z, alpha=0.3, cmap='winter')  # 生成表面， alpha 用于控制透明度
        ax4.contour(x1, x2, z, zdir='z', offset=-3, cmap="rainbow")  # 生成z方向投影，投到x-y平面
        ax4.contour(x1, x2, z, zdir='x', offset=-6, cmap="rainbow")  # 生成x方向投影，投到y-z平面
        ax4.contour(x1, x2, z, zdir='y', offset=6, cmap="rainbow")  # 生成y方向投影，投到x-z平面
        ax4.contourf(x1, x2, z, zdir='y', offset=6, cmap="rainbow")  # 生成y方向投影填充，投到x-z平面，contourf()函数
        ax4.scatter(min_x1,min_x2,min_z,c='r')
        # 设定显示范围
        ax4.set_xlabel('X')
        ax4.set_ylabel('Y')
        ax4.set_zlabel('Z')
        plt.show()

    # 开始
    def start(self):
        times = int(input("请输入需要随机优化的次数："))
        alpha = float(input("请输入随机优化的步长"))
        z = self.data()
        start_time = time.time()
        for i in range(times):
            answer = self.snt(self.x1,self.x2,z,alpha)
            self.answers.append(answer)
        min_answer = np.array(self.answers)
        for i in range(times):
            self.min_answer_z.append((1-min_answer[i,0,0])**2+(min_answer[i,1,0]-min_answer[i,0,0]**2)**2)
        optimal_z = np.min(np.array(self.min_answer_z))
        optimal_z_index = np.argmin(np.array(self.min_answer_z))
        optimal_x1,optimal_x2 = min_answer[optimal_z_index,0,0],min_answer[optimal_z_index,1,0]
        end_time = time.time()
        running_time = end_time-start_time
        print("优化的时间:%.2f秒!" % running_time)
        self.plot_data(optimal_x1,optimal_x2,optimal_z)
if __name__ == '__main__':
    snt = Rosenbrock()
    snt.start() 
```

```
请输入需要随机优化的次数：100 
```

请输入随机优化的步长0.01

优化的时间:8.10秒!

![](../img/f1c1ce5601be2993f870f630152a2c20.png)

*本文电子版 后台回复 ****优化算法****获取*

![](../img/ac1260bd6d55ebcd4401293b8b1ef5ff.png)

“整理不易，**点****赞****三连**↓