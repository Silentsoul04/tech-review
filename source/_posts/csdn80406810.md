---
title: "面向机器学习的特征工程 七、非线性特征提取和模型堆叠"
id: csdn80406810
---

# 七、非线性特征提取和模型堆叠

> 来源：[ApacheCN《面向机器学习的特征工程》翻译项目](https://github.com/apachecn/feature-engineering-for-ml-zh)
> 
> 译者：[friedhelm739](https://github.com/friedhelm739)
> 
> 校对：（虚位以待）

当在数据一个线性子空间像扁平饼时 PCA 是非常有用的。但是如果数据形成更复杂的形状呢？一个平面（线性子空间）可以推广到一个 *流形* （非线性子空间），它可以被认为是一个被各种拉伸和滚动的表面。

如果线性子空间是平的纸张，那么卷起的纸张就是非线性流形的例子。你也可以叫它瑞士卷。（见图 7-1），一旦滚动，二维平面就会变为三维的。然而，它本质上仍是一个二维物体。换句话说，它具有低的内在维度，这是我们在“直觉”中已经接触到的一个概念。如果我们能以某种方式展开瑞士卷，我们就可以恢复到二维平面。这是非线性降维的目标，它假定流形比它所占据的全维更简单，并试图展开它。

![图7-1](../img/f3903baca25a14add8e386c46cc12a27.png)

关键是，即使当大流形看起来复杂，每个点周围的局部邻域通常可以很好地近似于一片平坦的表面。换句话说，他们学习使用局部结构对全局结构进行编码。非线性降维也被称为非线性嵌入，或流形学习。非线性嵌入可有效地将高维数据压缩成低维数据。它们通常用于 2-D 或 3-D 的可视化。

然而，特征工程的目的并不是要使特征维数尽可能低，而是要达到任务的正确特征。在这一章中，正确的特征是代表数据空间特征的特征。

聚类算法通常不是局部结构化学习的技术。但事实上也可以用他们这么做。彼此接近的点（由数据科学家使用某些度量可以定义的“接近度”）属于同一个簇。给定聚类，数据点可以由其聚类成员向量来表示。如果簇的数量小于原始的特征数，则新的表示将比原始的具有更小的维度；原始数据被压缩成较低的维度。

与非线性嵌入技术相比，聚类可以产生更多的特征。但是如果最终目标是特征工程而不是可视化，那这不是问题。

我们将提出一个使用 k 均值聚类算法来进行结构化学习的思想。它简单易懂，易于实践。与非线性流体降维相反，k 均值执行非线性流形特征提取更容易解释。如果正确使用它，它可以是特征工程的一个强大的工具。

## [阅读全文](https://github.com/apachecn/feature-engineering-for-ml-zh/blob/master/docs/7.%E9%9D%9E%E7%BA%BF%E6%80%A7%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%A0%86%E5%8F%A0.md)