---
title: "DeepLearningAI 学习笔记 1.3 浅层 logistic 神经网络"
id: csdn78628917
---

# 1.3 浅层 logistic 神经网络

> 视频：[第三周 浅层神经网络](https://mooc.study.163.com/learn/deeplearning_ai-2001281002?tid=2001392029)
> 
> 整理：[飞龙](https://github.com/wizardforcel)

普通的 logistic 可看做无隐层的神经网络。下面我们做出一个单隐层的神经网络，它本质上是 logistic 套着 logistic，所以也叫作多层 logistic。

我们的神经网络有三层，输入层，一个隐层，和输出层。输入层的每个节点对应训练集`X`的每个特征，节点数量就是特征数量。隐层的节点任意，这张图里面是四个。输出层只有一个节点，它就是我们的假设。

![](../img/625cb4a5f7e03863b512eef16e3414e1.png)

每个隐层节点，以及输出层节点中，都要执行上一节的 logistic 运算。

上一节中，我们已经推导了向量化的公式。为了简便起见，我们直接用向量化的公式起步。

我们引入一种的表达方式，用 Z[1]j  表示隐层第`j`个节点里面的值。用 Z[2]  表示输出层里面的值，因为只有一个节点，就不加下标了。

在每个隐层节点中，我们有：

Z[1]j=Xθ[1]jA[1]j=σ(Z[1]j)

> 注：
> 
> 我这里的 X  仍然是行为样本，列为特征。如果你的 <nobr>X</nobr> 是我这里的转置，记得把其它的量也加上转置。

然后，我们尝试进一步使其向量化。

Θ[1]=⎡⎣⎢⎢⎢⎢⎢⎢⎢⋯|θ[1]j|⋯⎤⎦⎥⎥⎥⎥⎥⎥⎥

我们把 θ[1]j  按列堆叠，得到 Θ[1] 。由于 θ[1]j  是矩阵乘法的右边，它乘以 X  会得到按列堆叠的 <nobr>Z[1]j</nobr>。

Z[1]=XΘ[1]=⎡⎣⎢⎢⎢⎢⎢⎢⎢⋯|Z[1]j|⋯⎤⎦⎥⎥⎥⎥⎥⎥⎥

A[1]  就是对 Z[1]  的每个元素应用 sigmoid 函数，所以是一样的结构。

A[1]=σ(Z[1])=⎡⎣⎢⎢⎢⎢⎢⎢⎢⋯|A[1]j|⋯⎤⎦⎥⎥⎥⎥⎥⎥⎥

在神经网络中，sigmoid 函数叫做激活函数， A[1]  叫做激活值。每个节点的激活值提供给下一层，作为下一层的特征。

也就是说：

Z[2]=A[1]θ[2]A[2]=σ(Z[2])

A[2]  就是我们的假设，它等于样本属于正向分类的概率。

成本函数 J  的计算也类似。

<nobr>J=−Sum(Y∗log(A[2])+(1−Y)∗log(1−(A[2]))</nobr>

## 计算图

由于目前为止的量有点多，我们需要画出它们的关系图。

```
X-----------Z^[1]----A^[1]-------Z^[2]----A^[2]---J
            |                    |                |
Theta^[1]---+        theta^[2]---+        Y-------+
```

然后我们统计一下这些量的尺寸信息。

| 量 | 尺寸 |
| --- | --- |
| X  | `n_data x n_features` |
| <nobr>Θ[1]</nobr> | `n_features x n_hidden_nodes` |
| Z[1]  A[1]  | `n_data x n_hidden_nodes` |
| θ[2]  | `n_hidden_nodes x 1` |
| Z[2]  A[2]  | `n_data x 1` |

这个很重要，以后有用。

## 反向传播

神经网络中的求导过程又叫做反向传播，只是一个新名词，没什么特别的。

我们这里待定的量变成了两个： Θ[1]  和 θ[2] 。

首先， J  和 <nobr>θ[2]</nobr> 的关系，类似于 logistic 里面它和 θ  的关系。我们可以直接得出：

dJdθ[2]=A[1]T(A[2]−Y)

下面求 dJdΘ[1] 。从 J  到 <nobr>Θ[1]</nobr> 路径上的所有导数都需要求出来。首先我们得出：

dJdZ[2]=A[2]−Y

然后：

dZ[2]dA[1]=θ[2]T

这个导数与 A[1]  同型，只有我们将 θ[2]  转置过来，再广播成`n_data x n_hidden_nodes`，它才同型。

dJdA[1]=dJdZ[2]θ[2]T

我们发现，左边的导数是`n_data x n_hidden_nodes`的，右边的两个导数分别是`n_data x 1`和`1 x n_hidden_nodes`的，所以用矩阵乘法。

dA[1]dZ[1]=A[1]∗(1−A[1])dJdZ[1]=dJdA[1]∗A[1]∗(1−A[1])

我们发现，左边的导数是`n_data x n_hidden_nodes`的，右边的两个导数也是，所以用逐元素乘法。这个规律在反向传播中十分重要。

最后一步和 logistic 中的情况相似，所以照搬。

dJdΘ[1]=XTdJdZ[1]

最后别忘了对两个导数除以 ndata 。

## 代码

```
Theta_sup1 = np.random.rand(n_features, n_hidden_nodes) / 100
theta_sup2 = np.random.rand(n_hidden_nodes, 1) / 100

for _ in range(max_iter):
    # 正向传播过程
    Z_sup1 = np.dot(X, Theta_sup1)
    A_sup1 = sigmoid(Z_sup1)
    Z_sup2 = np.dot(A_sup1, theta_sup2)
    A_sup2 = sigmoid(Z_sup2)

    # 反向传播过程
    dJ_dZ_sup2 = (A_sup2 - Y) / n_data
    dJ_dtheta_sup2 = np.dot(A_sup1.T, dJ_dZ_sup2)
    dZ_sup2_dA_sup1 = theta_sup2.T
    dA_sup1_dZ_sup1 = A_sup1 * (1 - A_sup1)
    dJ_dZ_sup1 = np.dot(dJ_dZ_sup2, dZ_sup2_dA_sup1) * dA_sup1_dZ_sup1
    dJ_dTheta_sup1 = np.dot(X.T, dJ_dZ_sup1)

    Theta_sup1 -= alpha * dJ_dTheta_sup1
    theta_sup2 -= alpha * dJ_dtheta_sup2
```