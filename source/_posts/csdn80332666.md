---
title: 《Scikit-Learn与TensorFlow机器学习实用指南》第11章 训练深层神经网络
id: csdn80332666
---

# 第11章 训练深层神经网络

> 来源：[ApacheCN《Sklearn 与 TensorFlow 机器学习实用指南》翻译项目](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF)
> 
> 译者：[@akonwang](https://github.com/wangxupeng) [@飞龙](https://github.com/wizardforcel)
> 
> 校对：[@飞龙](https://github.com/wizardforcel) [@Zeyu Zhong](https://github.com/zhearing)

第 10 章介绍了人工神经网络，并训练了我们的第一个深度神经网络。 但它是一个非常浅的 DNN，只有两个隐藏层。 如果你需要解决非常复杂的问题，例如检测高分辨率图像中的数百种类型的对象，该怎么办？ 你可能需要训练更深的 DNN，也许有 10 层，每层包含数百个神经元，通过数十万个连接来连接。 这不会是闲庭信步：

*   首先，你将面临棘手的梯度消失问题（或相关的梯度爆炸问题），这会影响深度神经网络，并使较低层难以训练。
*   其次，对于如此庞大的网络，训练将非常缓慢。
*   第三，具有数百万参数的模型将会有严重的过拟合训练集的风险。

在本章中，我们将依次讨论这些问题，并提出解决问题的技巧。 我们将从解释梯度消失问题开始，并探讨解决这个问题的一些最流行的解决方案。 接下来我们将看看各种优化器，与普通梯度下降相比，它们可以加速大型模型的训练。 最后，我们将浏览一些流行的大型神经网络正则化技术。

使用这些工具，你将能够训练非常深的网络：欢迎来到深度学习的世界！

## [阅读全文](https://github.com/apachecn/hands-on-ml-zh/blob/master/docs/11.%E8%AE%AD%E7%BB%83%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.md)