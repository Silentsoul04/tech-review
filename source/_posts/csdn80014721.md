---
title: "《Scikit-Learn与TensorFlow机器学习实用指南》第7章 集成学习和随机森林"
id: csdn80014721
---

# 第7章 集成学习与随机森林

> 来源：[ApacheCN《Sklearn 与 TensorFlow 机器学习实用指南》翻译项目](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF)

> 译者：[@friedhelm739](https://github.com/friedhelm739)

> 校对：[@飞龙](https://github.com/wizardforcel)

假设你去随机问很多人一个很复杂的问题，然后把它们的答案合并起来。通常情况下你会发现这个合并的答案比一个专家的答案要好。这就叫做*群体智慧*。同样的，如果你合并了一组分类器的预测（像分类或者回归），你也会得到一个比单一分类器更好的预测结果。这一组分类器就叫做集成；因此，这个技术就叫做集成学习，一个集成学习算法就叫做集成方法。

例如，你可以训练一组决策树分类器，每一个都在一个随机的训练集上。为了去做预测，你必须得到所有单一树的预测值，然后通过投票（例如第六章的练习）来预测类别。例如一种决策树的集成就叫做随机森林，它除了简单之外也是现今存在的最强大的机器学习算法之一。

向我们在第二章讨论的一样，我们会在一个项目快结束的时候使用集成算法，一旦你建立了一些好的分类器，就把他们合并为一个更好的分类器。事实上，在机器学习竞赛中获得胜利的算法经常会包含一些集成方法。

在本章中我们会讨论一下特别著名的集成方法，包括 *bagging, boosting, stacking*，和其他一些算法。我们也会讨论随机森林。

## [阅读全文](https://github.com/apachecn/hands-on-ml-zh/blob/master/docs/7.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.md)