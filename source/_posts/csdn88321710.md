---
title: PyTorch 1.0 中文文档：torch.utils.checkpoint
id: csdn88321710
---

> 译者: [belonHan](https://github.com/belonHan)

注意

checkpointing的实现方法是在向后传播期间重新运行已被checkpint的前向传播段。 所以会导致像RNG这类(模型)的持久化的状态比实际更超前。默认情况下，checkpoint包含了使用RNG状态的逻辑(例如通过dropout)，与non-checkpointed传递相比,checkpointed具有更确定的输出。RNG状态的存储逻辑可能会导致一定的性能损失。如果不需要确定的输出，设置全局标志(global flag) `torch.utils.checkpoint.preserve_rng_state=False` 忽略RNG状态在checkpoint时的存取。

```
torch.utils.checkpoint.checkpoint(function, *args) 
```

checkpoint模型或模型的一部分

checkpoint通过计算换内存空间来工作。与向后传播中存储整个计算图的所有中间激活不同的是，checkpoint不会保存中间激活部分，而是在反向传递中重新计算它们。它被应用于模型的任何部分。

具体来说，在正向传播中，`function`将以`torch.no_grad()`方式运行 ，即不存储中间激活,但保存输入元组和 `function`的参数。在向后传播中，保存的输入变量以及 `function`会被取回，并且`function`在正向传播中被重新计算.现在跟踪中间激活，然后使用这些激活值来计算梯度。

Warning
警告

Checkpointing 在 [`torch.autograd.grad()`](autograd.html#torch.autograd.grad "torch.autograd.grad")中不起作用, 仅作用于 [`torch.autograd.backward()`](autograd.html#torch.autograd.backward "torch.autograd.backward").

警告

如果function在向后执行和前向执行不同，例如,由于某个全局变量，checkpoint版本将会不同，并且无法被检测到。

参数:

*   **function** - 描述在模型的正向传递或模型的一部分中运行的内容。它也应该知道如何处理作为元组传递的输入。例如，在LSTM中，如果用户通过 ，应正确使用第一个输入作为第二个输入(activation, hidden)functionactivationhidden
*   **args** – 包含输入的元组function

> [**阅读全文／改进本文**](https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.0/checkpoint.md)