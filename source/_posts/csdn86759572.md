---
title: "PyTorch 1.0 中文官方教程：对抗性示例生成"
id: csdn86759572
---

> 译者：[cangyunye](https://github.com/cangyunye)

**作者:** [Nathan Inkawhich](https://github.com/inkawhich)

如果你正在阅读这篇文章，希望你能理解一些机器学习模型是多么有效。现在的研究正在不断推动ML模型变得更快、更准确和更高效。然而，在设计和训练模型中经常会忽视的是安全性和健壮性方面，特别是在面对欺骗模型的对手时。

本教程将提高您对ML模型安全漏洞的认识，并将深入探讨对抗性机器学习这一热门话题。您可能会惊讶地发现，在图像中添加细微的干扰会导致模型性能的巨大差异。鉴于这是一个教程，我们将通过一个图像分类器上的示例来探索这个主题。具体来说，我们将使用第一个也是最流行的攻击方法之一，快速梯度符号攻击`Fast Gradient Sign Attack`(FGSM)，以欺骗一个MNIST分类器。

## 威胁模型

就上下文而言，有许多类型的对抗性攻击，每一类攻击都有不同的目标和对攻击者知识的假设。然而，总的目标是在输入数据中添加最少的扰动，以导致所需的错误分类。攻击者的知识有几种假设，其中两种是:**白盒**和**黑盒**。白盒攻击假定攻击者具有对模型的全部知识和访问权，包括体系结构、输入、输出和权重。黑盒攻击假设攻击者只访问模型的输入和输出，对底层架构或权重一无所知。目标也有几种类型，包括**错误分类**和**源/目标错误分类**。错误分类的目标意味着对手只希望输出分类是错误的，而不关心新的分类是什么。源/目标错误分类意味着对手想要更改原来属于特定源类的图像，以便将其分类为特定的目标类。

在这种情况下，FGSM攻击是一种以错误分类为目标的白盒攻击。有了这些背景信息，我们现在可以详细讨论攻击。

## 快速梯度符号攻击

到目前为止，最早也是最流行的对抗性攻击之一被称为快速梯度符号攻击(FGSM)，由Goodfellow等人在解释和利用对抗性示例( [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572))时介绍到。这种攻击非常强大，而且直观。它被设计用来攻击神经网络，利用他们学习的方式，梯度`gradients`。这个想法很简单，比起根据后向传播梯度来调整权重使损失最小化，这种攻击是根据相同的反向传播梯度调整输入数据来最大化损失。换句话说，攻击使用了输入数据相关的梯度损失方式，通过调整输入数据，使损失最大化。

在我们深入代码之前，让我们看看著名的[FGSM](https://arxiv.org/abs/1412.6572) panda示例并提取一些符号。

![fgsm_panda_image](../img/3fc40a3f99c76e1b57529528ea22cc02.png)

从图像中看，`\(\mathbf{x}\)` 是一个正确分类为“熊猫”(panda)的原始输入图像， `\(y\)` 是对`\(\mathbf{x}\)`的真实表征标签`ground truth label`, `\(\mathbf{\theta}\)` 表示模型参数， 而 `\(J(\mathbf{\theta}, \mathbf{x}, y)\)` 是用来训练网络的损失函数。 这种攻击将梯度后向传播到输入数据来计算 `\(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)\)`。然后将输入数据通过一小步(`\(\epsilon\)` 或 如图中的`\(0.007\)` ) 在(i.e. `\(sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y))\)`) 方向上调整，使损失最大化。结果将得到受到干扰的图像， `\(x'\)`，尽管图片还是“熊猫”，但它一杯目标网络错误分类为“长臂猿”(gibbon)了

希望看到现在的你，已经明确了解了本教程的动机，那么，让我们开始实现它吧。

> [**阅读全文／改进本文**](https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.0/fgsm_tutorial.md)