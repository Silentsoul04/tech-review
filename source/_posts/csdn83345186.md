---
title: "【番外】线性回归和逻辑回归的 MLE 视角"
id: csdn83345186
---

## 线性回归

令 z = w T x + b z = w^T x + b z=wTx+b，得到：

y = z + ϵ , &ThinSpace; ϵ ∼ N ( 0 , σ 2 ) y = z + \epsilon, \, \epsilon \sim N(0, \sigma^2) y=z+ϵ,ϵ∼N(0,σ2)

于是：

y ∣ x ∼ N ( z , σ 2 ) y|x \sim N(z, \sigma^2) y∣x∼N(z,σ2)

为啥是 y ∣ x y|x y∣x，因为判别模型的输出只能是 y ∣ x y|x y∣x。

它的概率密度函数：

f Y ∣ X ( y ) = 1 2 π σ exp ⁡ ( − ( y − z ) 2 2 σ 2 ) = A exp ⁡ ( − B ( y − z ) 2 ) , &ThinSpace; A , B &gt; 0 f_{Y|X}(y)=\frac{1}{\sqrt{2 \pi} \sigma} \exp(\frac{-(y -z)^2}{2\sigma^2}) \\ = A \exp(-B (y - z)^2), \, A, B &gt; 0 fY∣X​(y)=2π ​σ1​exp(2σ2−(y−z)2​)=Aexp(−B(y−z)2),A,B>0

计算损失函数：

L = − ∑ i log ⁡ f Y ∣ X ( y ( i ) ) = − ∑ i ( log ⁡ A − B ( y ( i ) − z ( i ) ) 2 ) = B ∑ i ( y ( i ) − z ( i ) ) 2 + C L = -\sum_i \log f_{Y|X}(y^{(i)}) \\ = -\sum_i(\log A - B(y^{(i)} - z^{(i)})^2) \\ = B \sum_i(y^{(i)} - z^{(i)})^2 + C L=−∑i​logfY∣X​(y(i))=−∑i​(logA−B(y(i)−z(i))2)=B∑i​(y(i)−z(i))2+C

所以 min ⁡ L \min L minL 就相当于 min ⁡ ( y ( i ) − z ( i ) ) 2 \min (y^{(i)} - z^{(i)})^2 min(y(i)−z(i))2。结果和最小二乘是一样的。

## 逻辑回归

令 z = w T x + b , a = σ ( z ) z = w^T x + b, a = \sigma(z) z=wTx+b,a=σ(z)，我们观察到在假设中：

P ( y = 1 ∣ x ) = a P ( y = 0 ∣ x ) = 1 − a P(y=1|x) = a \\ P(y=0|x) = 1 - a P(y=1∣x)=aP(y=0∣x)=1−a

也就是说：

y ∣ x ∼ B ( 1 , a ) y|x \sim B(1, a) y∣x∼B(1,a)

> 其实任何二分类器的输出都是伯努利分布。因为变量只能取两个值，加起来得一，所以只有一种分布。

它的概率质量函数（因为是离散分布，只有概率质量函数，不过无所谓）：

p Y ∣ X ( y ) = a y ( 1 − a ) 1 − y p_{Y|X}(y) = a^y(1-a)^{1-y} pY∣X​(y)=ay(1−a)1−y

然后计算损失函数：

L = − ∑ i log ⁡ p Y ∣ X ( y ( i ) ) = − ∑ i ( y ( i ) log ⁡ a ( i ) + ( 1 − y ( i ) ) log ⁡ ( 1 − a ( i ) ) ) L = -\sum_i \log p_{Y|X}(y^{(i)}) \\ = -\sum_i(y^{(i)} \log a^{(i)} + (1-y^{(i)})\log(1-a^{(i)})) L=−∑i​logpY∣X​(y(i))=−∑i​(y(i)loga(i)+(1−y(i))log(1−a(i)))

和交叉熵是一致的。

可以看出，在线性回归的场景下，MLE 等价于最小二乘，在逻辑回归的场景下，MLE 等价于交叉熵。但不一定 MLE 在所有模型中都是这样。